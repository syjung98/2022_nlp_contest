{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9cd8e2",
   "metadata": {},
   "source": [
    "## 모델구성 과정의 실험\n",
    "- 최종모델을 만들어나가는 과정에서 사용을 고려했던 코드 목록\n",
    "- 해당 코드들은 최종예측에는 사용되지 않았으나 실험에 사용되었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f27ba",
   "metadata": {},
   "source": [
    "### 전처리 ver1\n",
    "- 초기 전처리 과정에서 data_cleansing_text 함수 사용\n",
    "- 최종예측에서는 추가적인 전처리 작업 및 맞춤법 교정을 제외함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a337dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "from hanspell.constants import CheckResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing_text(df,col_lst):\n",
    "    for col in col_lst:\n",
    "        # null값 채우기\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "        \n",
    "        # 양쪽 공백 삭제\n",
    "        df[col] = df[col].apply(lambda x : x.strip())\n",
    "        \n",
    "        # 영어 대문자 -> 소문자\n",
    "        #df[col] = df[col].apply(lambda x : x.lower())\n",
    "        \n",
    "        #영어 문자 사이의 특수기호 삭제\n",
    "        df[col] = df[col].str.replace(pat=\"(?<=[a-z])[/$](?=[a-z])\",repl = \"\",regex=True)\n",
    "        \n",
    "        # ^ 삭제\n",
    "        df[col] = df[col].str.replace(\"^\",\"\")\n",
    "        \n",
    "        # 특수기호 -> 공백으로\n",
    "        #df[col] = df[col].apply(lambda x : re.sub('[^\\w\\s]', ' ', x))\n",
    "        \n",
    "        # 특정 특수기호 -> 공백으로\n",
    "        df[col] = df[col].apply(lambda x : re.sub(r'[!@#$%&*<>?\\|+-,./]', ' ', x))\n",
    "        #% 또는 $ 또는 ^ 또는 * 또는 !을 없애주는 것이다.\n",
    "        # ( ) 남기기\n",
    "        \n",
    "        # 숫자 지우기\n",
    "        df[col] = df[col].apply(lambda x : re.sub(r'[0-9-]','',x))\n",
    "        \n",
    "        # 다중공백 -> 단일공백\n",
    "        df[col] = df[col].apply(lambda x : re.sub(' +', ' ', x))\n",
    "        \n",
    "        # 한글 자음모음 삭제\n",
    "        df[col] = df[col].apply(lambda x : re.sub('([ㄱ-ㅎㅏ-ㅣ]+)',\"\",x))\n",
    "        \n",
    "        # ㎡ 변환\n",
    "        df[col] = df[col].apply(lambda x : re.sub('㎡',\"제곱미터\",x))\n",
    "        \n",
    "        # m2 변환\n",
    "        df[col] = df[col].apply(lambda x : re.sub('m2',\"제곱미터\",x))\n",
    "        \n",
    "        # m2 변환\n",
    "        df[col] = df[col].apply(lambda x : re.sub('M2',\"제곱미터\",x))\n",
    "        \n",
    "        # 맞춤법 검사기\n",
    "        df[col] = df[col].apply(lambda x : spell_checker.check(x).checked)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c98b2",
   "metadata": {},
   "source": [
    "### 중복처리\n",
    "- 텍스트와 label이 모두 중복되는 데이터 30만개이상 발견\n",
    "- 텍스트만 중복되고 label 같지 않은 데이터 발견\n",
    "- 해당 데이터들을 제외하고 모델링하는 것이 성능에 긍정적이지 않기에 전처리하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파일 불러오기\n",
    "import pandas as pd\n",
    "train_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/train_preprocessed_ver2_no_lower_no_ques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed[\"document\"] = train_preprocessed[\"document\"].apply(lambda x : x.strip())\n",
    "\n",
    "train_preprocessed = train_preprocessed.drop_duplicates([\"document\",\"label\"], keep='first', inplace=False, ignore_index=False).reset_index(drop=True)\n",
    "# document, label 중복 drop하기\n",
    "\n",
    "train_preprocessed = train_preprocessed.drop_duplicates([\"document\"],keep=False).sort_values(\"document\").reset_index(drop=True)\n",
    "# document만 중복인 것들까지 전부 drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7933d9e",
   "metadata": {},
   "source": [
    "### 역번역(Back Translation)을 통한 데이터 증강(Data Augmentation)\n",
    "- 예측할 label(표준산업분류코드)가 매우 imbalance한 것 EDA를 통해 확인함\n",
    "- 한글 -> 영어 -> 한글로 번역 과정을 통해 희소한 label을 증강하려고 함\n",
    "- 성능에 긍정적인 영향을 주지 못해 역번역을 통한 데이터 증강 중지\n",
    "- label 개수가 230개 미만인 Q1 데이터에 대해서 역번역 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921afd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tnrange\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def54892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrome_setting():\n",
    "  chrome_options = webdriver.ChromeOptions()\n",
    "  chrome_options.add_argument('--headless')\n",
    "  chrome_options.add_argument('--no-sandbox')\n",
    "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "  driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
    "  return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=chrome_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글드라이브 연동\n",
    "from google.colab import drive\n",
    "drive.mount('/content/MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파일 불러오기\n",
    "import pandas as pd\n",
    "train_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/train_preprocessed_ver2_no_lower_no_ques.csv\")\n",
    "test_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/test_preprocessed_ver2_no_lower_no_ques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff87acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역번역할 데이터 수집\n",
    "augment_label = train_preprocessed[\"label\"].value_counts()[train_preprocessed[\"label\"].value_counts() < 230].index\n",
    "mask = train_preprocessed['label'].isin(augment_label)\n",
    "augment_preprocessed = train_preprocessed[mask].reset_index(drop=True)\n",
    "augment_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bf8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어에서 다른 언어로 번역\n",
    "\n",
    "def kor_to_trans(text_data, trans_lang,start_index,final_index):\n",
    "    trans_list = []\n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "    for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "        if (i!=0)&(i%99==0):\n",
    "            time.sleep(2)\n",
    "            print('{}th : '.format(i), backtrans)\n",
    "    \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+text_data[i])\n",
    "            time.sleep(1.5)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.1)\n",
    "            backtrans = element.text \n",
    "\n",
    "            if (backtrans=='')|(backtrans==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans = element.text \n",
    "                trans_list.append(backtrans)\n",
    "            else:\n",
    "                trans_list.append(backtrans)\n",
    "        \n",
    "        except:\n",
    "            trans_list.append('')\n",
    "\n",
    "    return trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d457f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른언어에서 한국어로 번역\n",
    "\n",
    "def trans_to_kor(transed_list, transed_lang,start_index,final_index): \n",
    "  \n",
    "    trans_list = []\n",
    "    target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
    "\n",
    "    for i in tqdm(range(start_index,final_index)): \n",
    "    \n",
    "        if (i!=0)&(i%99==0):\n",
    "            time.sleep(1.5)\n",
    "            print('{}th : '.format(i), backtrans)\n",
    "    \n",
    "        try:\n",
    "            driver.get('https://papago.naver.com/?sk=en&tk='+transed_lang+'&st='+transed_list[i])\n",
    "            time.sleep(2)\n",
    "            element=WebDriverWait(driver, 10).until(target_present)\n",
    "            time.sleep(0.2)\n",
    "            backtrans = element.text \n",
    "\n",
    "            if (backtrans=='')|(backtrans==' '):\n",
    "                element=WebDriverWait(driver, 10).until(target_present)\n",
    "                backtrans = element.text\n",
    "                trans_list.append(backtrans)\n",
    "            else:\n",
    "                trans_list.append(backtrans)\n",
    "    \n",
    "        except:\n",
    "            trans_list.append('')\n",
    "    return trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a691a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 -> 영어 번역\n",
    "trans_list = kor_to_trans(augment_preprocessed['document'], 'en',0,len(augment_preprocessed))\n",
    "augment_preprocessed[\"eng\"] = augment_preprocessed\n",
    "\n",
    "# 영어 -> 한글 재번역\n",
    "trans_list = trans_to_kor(augment_preprocessed['eng'], 'en',0,len(augment_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb4496",
   "metadata": {},
   "source": [
    "### FocalLoss 함수\n",
    "- label이 불균형한 데이터 더 효과적인 FocalLoss 함수를 손실함수로 책정\n",
    "- CrossEntropyLoss 대신 FocalLoss 사용 시, 예측성능이 낮아지기에 CrossEntropyLoss 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha = 0.25, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        ce_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.2).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = FocalLoss()\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader_1) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "#train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f06001",
   "metadata": {},
   "source": [
    "### KLUE-BERT\n",
    "- 한국어 pre-trained 모델이며 벤치마크 데이터 셋을 가진 KLUE-BERT 모델로 예측진행\n",
    "- small/base/large 모델 다양한 실험 진행\n",
    "- 예측성능이 KoBERT 모델보다 높지 않기에 KoBERT 최종모델로 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "pip install transformers\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파일 불러오기\n",
    "import pandas as pd\n",
    "train_preprocessed = pd.read_csv(\"test/train_preprocessed_ver2_no_lower_no_ques.csv\")\n",
    "test_preprocessed = pd.read_csv(\"test/test_preprocessed_ver2_no_lower_no_ques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db4fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 1개인 것들 제거\n",
    "except_label = train_preprocessed[\"label\"].value_counts()[train_preprocessed[\"label\"].value_counts() < 2].index\n",
    "mask = train_preprocessed['label'].isin(except_label)\n",
    "train_preprocessed = train_preprocessed[~mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c04891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train_preprocessed[\"label\"])\n",
    "\n",
    "LABEL = le.transform(train_preprocessed[\"label\"])\n",
    "train_preprocessed[\"encoded_cat\"] = LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902269b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6838cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링을 위한 데이터 convert\n",
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ef695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 검증용 데이터 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_preprocessed['document'], train_preprocessed['encoded_cat'],\n",
    "                                                  test_size = 0.2, \n",
    "                                                  random_state = 777,\n",
    "                                                  stratify = train_preprocessed['encoded_cat'],\n",
    "                                                  shuffle = True)\n",
    "\n",
    "# 데이터 convert\n",
    "max_seq_len = 64\n",
    "\n",
    "train_X, train_y = convert_examples_to_features(train_X, train_y, max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "val_X, val_y = convert_examples_to_features(val_X, val_y, max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer,AdamWeightDecay,TFRobertaModel,TFBertModel\n",
    "#model = TFRobertaModel.from_pretrained(\"klue/roberta-large\", from_pt=True)\n",
    "#model = TFRobertaModel.from_pretrained(\"klue/roberta-small\", from_pt=True)\n",
    "model = TFRobertaModel.from_pretrained(\"klue/roberta-base\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이어 설정\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "token_type_ids_layer = tf.keras.layers.Input(shape=(max_seq_len,), dtype=tf.int32)\n",
    "outputs = model([input_ids_layer, attention_masks_layer, token_type_ids_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9730ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델 생성\n",
    "class Klue_RobertaClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_class):\n",
    "        super(Klue_RobertaClassifier, self).__init__()\n",
    "        #self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-large\", from_pt=True)\n",
    "        #self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-small\", from_pt=True)\n",
    "        self.bert = TFRobertaModel.from_pretrained(\"klue/roberta-base\", from_pt=True)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range,seed=42), \n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask, token_type_ids = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls_token = outputs[1]\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 223\n",
    "model = Klue_RobertaClassifier(num_class=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(1e-5,weight_decay_rate=1e-4)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d644b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버피팅 방지를 위한 EarlyStopping\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = F\"/content/MyDrive/MyDrive/test/\"\n",
    "checkpoint_path = os.path.join(model_path,'weight_klue_roberta_large_add.h5')\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습진행\n",
    "model.fit(train_X, train_y, epochs=2, batch_size=64, validation_data = (val_X,val_y), callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측진행\n",
    "test_preprocessed[\"label\"] = 0\n",
    "test_X, test_y = convert_examples_to_features(test_preprocessed['document'], test_preprocessed['label'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "# 예측값 디코딩\n",
    "PRED_LABEL = le.inverse_transform(pred_label)\n",
    "test_preprocessed[\"label\"] = PRED_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4af722",
   "metadata": {},
   "source": [
    "### DistillKoBERT + 딥러닝 모델\n",
    "- DistillKoBERT 모델에 딥러닝 모델을 얹어서 실험\n",
    "- 예측성능이 KoBERT 모델보다 높지 않기에 KoBERT 최종모델로 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/content/drive/MyDrive/additional_package'\n",
    "\n",
    "!pip install --target=$my_path transformers # transformers 대신에 원하는 패키지 이름을 넣으시면 됩니다\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/additional_package')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm # Progress Bar\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import warnings\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error() # Hidding Huggingface Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "except_label = train_preprocessed[\"label\"].value_counts()[train_preprocessed[\"label\"].value_counts() < 230].index\n",
    "mask = train_preprocessed['label'].isin(except_label)\n",
    "train_preprocessed = train_preprocessed[~mask].reset_index(drop=True)\n",
    "train_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파일 불러오기\n",
    "import pandas as pd\n",
    "train_preprocessed = pd.read_csv(\"test/train_preprocessed_ver2_no_lower_no_ques.csv\")\n",
    "test_preprocessed = pd.read_csv(\"test/test_preprocessed_ver2_no_lower_no_ques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02200080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 개수 230개 미만인 것 삭제\n",
    "except_label = train_preprocessed[\"label\"].value_counts()[train_preprocessed[\"label\"].value_counts() < 230].index\n",
    "mask = train_preprocessed['label'].isin(except_label)\n",
    "train_preprocessed = train_preprocessed[~mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 8:2로 분할 (train set, val set)\n",
    "train_df = train_preprocessed.sample(frac=0.8, random_state =1)\n",
    "val_df = train_preprocessed.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a00544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "le = LabelEncoder()\n",
    "train_df['encoded_label'] = le.fit_transform(train_df['label'])\n",
    "val_df['encoded_label'] = le.transform(val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34951082",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = (val_df[['label','encoded_label']].drop_duplicates()\n",
    "              .sort_values(by='encoded_label')\n",
    "              .reset_index(drop=True)['label']\n",
    "              .to_dict())\n",
    "\n",
    "for index, key in label_dict.items():\n",
    "    print(index, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_transformers.tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/distilkobert') # monologg/distilkobert도 동일\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = KoBertTokenizer.from_pretrained(MODEL_NAME) # Loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df['document']\n",
    "y_train = train_df['encoded_label']\n",
    "x_val = val_df['document']\n",
    "y_val = val_df['encoded_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb765b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0\n",
    "for sent in (x_train.tolist() + x_train.tolist() + x_val.tolist()):\n",
    "    try:\n",
    "        sent_tok_len = len(tokenizer.tokenize(sent))\n",
    "        max_val = sent_tok_len if (sent_tok_len > max_val) else max_val\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(f\"The maximum amount of tokens in the dataset is {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 27\n",
    "from kobert_transformers.tokenization_kobert import KoBertTokenizer\n",
    "tokenizer = KoBertTokenizer.from_pretrained(MODEL_NAME,                                              \n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=MAX_LENGTH, \n",
    "                                            pad_to_max_length=True) # monologg/distilkobert도 동일\n",
    "\n",
    "def tokenize(sentences, tokenizer):\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for sentence in tqdm(sentences):\n",
    "        inputs = tokenizer.encode_plus(sentence, \n",
    "                                       add_special_tokens=True, \n",
    "                                       max_length=MAX_LENGTH, \n",
    "                                       pad_to_max_length=True, \n",
    "                                       return_attention_mask=True, \n",
    "                                       return_token_type_ids=True, \n",
    "                                       truncation=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenize(x_train, tokenizer)\n",
    "x_test = test_df['document']\n",
    "X_test = tokenize(x_test, tokenizer)\n",
    "X_val = tokenize(x_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e69ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_num = 10\n",
    "\n",
    "print(train_df.iloc[index_num]['label'], '\\n')\n",
    "\n",
    "print(x_train.iloc[index_num], '\\n')\n",
    "temp_tokens = tokenizer.tokenize(x_train.iloc[index_num])\n",
    "temp_ids = tokenizer.encode(x_train.iloc[index_num])\n",
    "\n",
    "print('ID\\'s', 'Input Tokens', sep='\\t')\n",
    "for i in range(len(temp_ids)):\n",
    "    if i == 0:\n",
    "        print(temp_ids[i], '[CLS]', sep='\\t')\n",
    "        continue\n",
    "    if i == len(temp_ids)-1:\n",
    "        print(temp_ids[i], '[SEP]', sep='\\t')\n",
    "        break\n",
    "    print(temp_ids[i], temp_tokens[i-1], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f59ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=True, output_attentions=True)\n",
    "DistilBERT = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='masked_token', dtype='int32') \n",
    "\n",
    "embedding_layer = DistilBERT(input_ids = input_ids_in, attention_mask = input_masks_in)[0]\n",
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(embedding_layer)\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "X = tf.keras.layers.Dense(1024, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "X = tf.keras.layers.Dense(169, activation='softmax')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
    "\n",
    "# for layer in model.layers[:3]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(filepath=output_dir+'/weights.{epoch:02d}.hdf5',\n",
    "                                  save_weights_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, # Stop after 3 epochs of no improvement\n",
    "                               monitor='val_loss', # Look at validation_loss\n",
    "                               min_delta=0, # After 0 change\n",
    "                               mode='min', # Stop when quantity has stopped decreasing\n",
    "                               restore_best_weights=False, # Don't Restore the best weights\n",
    "                               verbose=1) \n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', # Look at validation loss\n",
    "                              min_lr=0.000001, # Lower bound of learning rate\n",
    "                              patience=1, # Reduce after 1 with little change\n",
    "                              mode='min', # Stop when quantity has stopped decreasing\n",
    "                              factor=0.1, # Reduce by a factor of 1/10\n",
    "                              min_delta=0.01, # Minimumn change needed\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf88a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    epochs = 3,\n",
    "                    batch_size=16, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[model_checkpoint, early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55461cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    print(\"Lowest Validation Loss: epoch {}\".format(np.argmin(val_loss)+1))\n",
    "    print(\"Highest Validation Accuracy: epoch {}\".format(np.argmax(val_acc)+1))\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_val_loss_epoch(history):\n",
    "    return \"0\"+str(np.argmin(history.history['val_loss'])+1)\n",
    "\n",
    "def get_max_val_acc_epoch(history):\n",
    "    return \"0\"+str(np.argmax(history.history['val_accuracy'])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293bfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = get_max_val_acc_epoch(history)\n",
    "model.load_weights(output_dir+\"/weights.\"+epoch_num+\".hdf5\") # Load in model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea58c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_probs = model.predict(X_test)\n",
    "\n",
    "# Turn probabilities into an interger prediction\n",
    "y_hat = []\n",
    "for prob in y_test_probs:\n",
    "    y_hat.append(np.argmax(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_hat))\n",
    "print_cf1(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ece7d",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- BERT가 아닌 임베딩 모델인 FastText 사용한 예측 진행\n",
    "- FastText와 딥러닝 모델을 함께 이용한 예측도 진행\n",
    "- 예측성능이 KoBERT 모델보다 높지 않기에 KoBERT을 최종모델로 채택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198cf6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23adb828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 파일 불러오기\n",
    "import pandas as pd\n",
    "train_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/train_preprocessed_ver2_no_lower_no_ques.csv\")\n",
    "test_preprocessed = pd.read_csv(\"/content/MyDrive/MyDrive/test/test_preprocessed_ver2_no_lower_no_ques.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aada31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext 사용 용이하게 하기 위해 txt로 저장했다가 다시 불러옴\n",
    "train_preprocessed.to_csv('df2.txt', sep = '\\t', index = False)\n",
    "labeling = pd.read_csv(\"/content/df2.txt\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0db9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(\"/content/df2.txt\", wordNgrams=3, epoch=15, lr=0.35,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for line in test_df['document']:\n",
    "    pred_label = model.predict(line, threshold=0.2)[0]\n",
    "    predictions.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anslist = []\n",
    "for i in predictions:\n",
    "    try:\n",
    "        i = i[0].replace('_','').replace('label','')\n",
    "        #print(i)\n",
    "        anslist.append(i)\n",
    "    except:\n",
    "        anslist.append(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b054ea5",
   "metadata": {},
   "source": [
    "자모단위 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8989b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj\n",
    "sample_text = \"가나다한글\"\n",
    "j2hcj(h2j(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3779240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "def word2jamo(sen):\n",
    "    return j2hcj(h2j(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265157f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed['jamo'] = train_preprocessed['document'].apply(word2jamo)\n",
    "train_preprocessed['jamo'].to_csv('fasttext_embedding_ver0.1_corpus.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\"fasttext_embedding_ver0.1_corpus.txt\", wordNgrams=3,model='skipgram', epoch=50, lr=0.05,verbose=0)\n",
    "model.save_model(\"fasttext_ver0.1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad72945",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list_train = []\n",
    "for i in train_preprocessed['jamo']:\n",
    "    vec = model.get_sentence_vector(i)\n",
    "    vec_list_train.append(vec)\n",
    "\n",
    "vec_list_test = []\n",
    "for i in test_preprocessed['jamo']:\n",
    "    vec = model.get_sentence_vector(i)\n",
    "    vec_list_test.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 230미만 삭제\n",
    "train_preprocessed['emb_vec'] = vec_list_train\n",
    "test_preprocessed['emb_vec'] = vec_list_test\n",
    "\n",
    "countdf = train_preprocessed.groupby('label').count().sort_values(by='document')\n",
    "over230labels = countdf[countdf['document']>230].index.to_list()\n",
    "train_preprocessed = train_preprocessed[train_preprocessed['label'].isin(over230labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c00e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_preprocessed['emb_vec'].to_list())\n",
    "y_train = np.array(train_preprocessed['label'].to_list())\n",
    "X_test = np.array(test_preprocessed['emb_vec'].to_list())\n",
    "\n",
    "np.save('X_train_ver0.1.npy', X_train)\n",
    "np.save('X_test_ver0.1.npy', X_test)\n",
    "np.save('y_train_ver0.1.npy', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#딥러닝 모델 구축 시작\n",
    "X_train = np.load('X_train_ver0.1.npy',allow_pickle=True)\n",
    "y_train = np.load('y_train_ver0.1.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74321c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58deee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_digit = encoder.transform(y_train)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train,y_train_digit, \n",
    "                                                    test_size=0.2,  \n",
    "                                                    random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f697d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model1 = Sequential(name = 'model1')\n",
    "model1.add(Dense(100, activation='relu', input_dim=(100)))\n",
    "model1.add(Dense(1024, activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(1024, activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(168, activation='softmax'))\n",
    "model1.summary()\n",
    "#plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True)\n",
    "history = model1.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential(name = 'model1')\n",
    "model2.add(Dense(100, activation='relu', input_dim=(100)))\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(168, activation='softmax'))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True)\n",
    "history_model2 = model2.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential(name = 'model3')\n",
    "model3.add(Dense(100, activation='relu', input_dim=(100)))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(168, activation='softmax'))\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True)\n",
    "history_model3 = model3.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True)\n",
    "history_model5 = model5.fit(x_train,y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    callbacks=early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19086534",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
